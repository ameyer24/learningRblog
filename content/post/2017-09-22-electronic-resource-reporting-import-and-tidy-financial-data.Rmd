---
title: Electronic Resource Reporting - Import and Tidy Financial Data
author: Andy Meyer
date: '2017-09-22'
categories:
  - Electronic Resources
tags:
  - data import
  - financial data
  - tidy
  - tidyverse
slug: electronic-resource-reporting-import-and-tidy-financial-data
---

Early in the electronic resource reporting process I realized that an essential component to this analysis would be the financial information. So I then had to think about 1) what financial information would be useful and 2) how to import and use that data in R.

Unlike standard counter reports - which have clear definitions and come in a standard format - the financial information for the electronic resource spending is not standardized and does not exist in one place. Therefore the data import really started as a data gathering process and, in some cases, as a data creation process. This was messy and the part of the project that feels the most "home-grown" and hacked together.

## Creating the Template

To make the data import process a little easier and to make the event matching process easier, I used data from database usage reports (stored as dataframe `DB1` in R) to create a template to start the process.
```{r eval=FALSE}
db.price.template <- DB1 %>%
  mutate(Year = year(Date)) %>%
  distinct(Database, Publisher, Platform, Year) %>%
  mutate(Price ="",
         Notes="",
         Fund="",
         Order_Agent="",
         Order_Term="Fiscal Year") %>%
  spread(Year,Price) %>%
  write_csv(paste(output.folder, "database.price.template.csv",sep="/"))
```

This chunk of code takes the tidy DB1 dataframe and reduces it to unique entries for every Database, Publisher, Platform, and Year. It then uses the mutate function to create empty columns for the following bits of data:

* Price
* Notes
* Fund
* Order_Agent
* Order_Term - for this variable the program sets the default to "Fiscal Year" because the majority of our electronic resources are renewed on the fiscal year.

Finally, it uses the spread function to turn the data from a long and tidy dataframe to wide dataframe that is easier for humans to read and input data into. Lastly, it writes this new wide dataframe to a CSV file.

I then manually entered data into this CSV file. It was laborous.

## Importing the Data

Once I entered the relevant data, R imports that file into a new dataframe. This dataframe is still "wide" and not "tidy" so the script with gather the cost information to return the dataframe to a tidy format.

```{r eval=FALSE}
db.prices.raw <- read_csv(paste(input.folder, "database.price.csv",sep="/"),col_names = TRUE)


DB1.fin <- db.prices.raw %>%
  gather(key = Fiscal_Year, value = Cost, 8:16) %>% 
  filter(!is.na(Cost)) %>%
  mutate(Cost = as.numeric(Cost))
```

It also filters out rows that lack cost information and converts the cost data into numeric values for other manipulations.

This dataframe, DB1.fin is now a tidy dataframe that contains the financial information for the databases.

## Odds and Ends

A few odds and ends to conclude these section:

* To include resources that aren't in the template, I  created additional rows in the CSV file before importing it back into R. These new lines won't correspond to an usage data but I decided it was worth including those because it made other types of reporting easier.
* If the financial data at your institution was consolidated in a central system (and could be mappped to database usage information) this step of importing and tidying could look much different. This manual approach was born of need; better source data would have a much better import process.
* Right now, this project imports usage information as well as financial information. Other information, like google 

---
title: Using R and the Tidyverse to Generate Library Usage Reports
author: Andy Meyer
date: '2017-10-24'
categories:
  - Electronic Resources
tags:
  - tidyverse
slug: using-r-and-the-tidyverse-to-generate-library-usage-reports
---

This article describes a project that used the statistical programming language R to generate reports about electronic usage and spending. This project had two main goals. First, to automate and systematize the process of gathering, compiling, and communicating usage information from electronic resource usage and information. Second, to increase library staff capacity for data science and create a starting point to share with other libraries. This project uses a set of R packages known as the “Tidyverse” and employs a general approach to data science that can be extended to manage and communicate data from other library systems such as circulation transactions, interlibrary loan usage, or log analysis.

## Introductions
With library budgets shrinking and the cost of online resources growing, many libraries must critically examine electronic resource usage and costs each year to make responsible collection development decisions. Yet gathering this information, making this data useable, and communicating it to a variety of stakeholders is a very difficult process. 

### R
R is an open source programming language freely available under the GNU General Public License. R excels in statistical computing and graphics and is highly extendable through additional packages. Along with Python, R is very popular in the fields of data science and data analytics. Additionally, because of R’s ability to create publication quality graphs, R is also popular in academia and among researchers. Growing in popularity and acceptance. Because of these factors, dedicating library staff time to learning R may help position the library to take on additional roles in data management, ease the work of scholarly publishing, and create new connections on campus

### Tidyverse
The tidyverse is set of R packages “share a common philosophy of data and R programming, and are designed to work together naturally.” (http://r4ds.had.co.nz/introduction.html)

### Counter Reports
COUNTER is a non-profit organization that provides libraries, publishers, and vendors a Code of Practice and set of definitions that facilitates the standard and consistent way to look at electronic resource usage. The project described in this paper focused on using the Database 1 reports generated under the fourth revision of Counter. However, the general approach outlined here could easily be extended to include other report types and/or reports from other revisions of the counter.

## Project Approach and Overview
This project approaches data science following the model proposed by Hadley Wickham in R for Data Science. Code is structured into three discrete steps:

1. Import and Tidy
2. Transform, Visualize, and Model
3. Communicate

![](http://r4ds.had.co.nz/diagrams/data-science.png)

This project has also adopted a functional approach that uses functions to do most of the work. This approach limits redundant code and are hopes to make the code transparent and easily shareable. Lastly, the code is written for clarity and simplicity; it is not written to optimize performance or to brevity. The hope is the the variable and function names as well as comments allow for others to easily use and adapt this code to their local institutional needs.

### Tidy Data
Before moving on, a note about language and terminology. In the context of the Tidyverse and throughout this paper, the word “tidy” carries a specific and technical definition.

"Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

Messy data is any other arrangement of the data."

Counter Reports, although standardized, are messy according to this definition. The tidying process starts by identifying the variables of these reports; for the database reports these variables are:

* Database
* Publisher
* Platform
* User Activity
* Date (expressed as Month-Year)
For Counter Reports, the observations are the usage for a given set of variables. Conveying this visually; variables are circled in red and the observations are noted in the green box.
(insert image here)

Conveying this data in a tidy dataframe would keep the Database, Publisher, Platform, and User Activity columns and add column for Date and Usage.
### Import and Tidy
The first step of this project is to import the data into a single tidy dataframe in R. This step of the process begins with a folder that contains a variety of reports from different vendors, in different file formats, spanning inconsistent date ranges and ends with a single, tidy dataframe.

#### Tidy the Data from Counter Reports
```{r echo=TRUE}
# Transforms data from the standard Counter Format to a tidy dataframe.
tidy.DB1reports <- function(df) {
  df %>%
    select(-c(5)) %>%
    gather(Date, Usage, -c(1:4)) %>%
    mutate(Date = as.yearmon(Date, "%b-%Y")) %>%
    mutate(Usage = as.numeric(Usage)) %>%
    rename("User_Activity" = "User Activity")
}
```
This function performs operations on a dataframe (df) to make it tidy. Walking through line by line:

1. `select(-c(5))` - Deletes the 5th column. The Reporting Period Total is not useful for this operation and is therefore excluded.
2. `gather(Date, Usage, -c(1:4))` - The gather function is the primary way of moving data from a "wide" format into a tidy "long" format. This step moves data from multiple columns (everything except for columns 1 to 4) and creates a new variable called "Date" that is defined as the key and a new column called "Usage" that is the value.
3. `mutate(Date = as.yearmon(Date, "%b-%Y"))` This mutates the Date column from a character variable to a YearMon data type. This is important for date operations later on.
4. `mutate(Usage = as.numeric(Usage))` - Changes the Usage values from character strings to numbers. Important for later transformations.
5. `rename("User_Activity" = "User Activity")` - Renames this column to avoid the space in the column name. Probably optional but made life easier while working with the data.

#### Import the Data from the Files

```{r echo=TRUE}
# This function imports data from CSV files and makes that data tidy.
csv.importer <- function(file) {
  file %>%
    read_csv(skip=7, col_names = TRUE) %>%
    tidy.DB1reports()
}
# This function imports data from Excel files and makes that data tidy.
excel.importer <- function(file) {
  file %>%
    read_excel(skip=7, col_names = TRUE) %>%
    tidy.DB1reports()
}
```

This simple functions each import a file by reading the file and then tidying the data using the function defined above. It is somewhat elegent to have two functions that basically do the same thing and I wish this section of code was better. However, this solution is effective, conceptually simple, and easily extendable.

### Loading and Tidying
The project now has functions that can tidy a Counter Report as well as load a particular file. We now need to complete the process by applying these functions to all the files in a given folder.

```{r echo=TRUE}
# These functions load DB1 reports from a given folder.
load.DB1.csv <- function(path) { 
  csv.files <- dir(path, pattern = "*.(CSV|csv)", full.names = TRUE)
  tables <- lapply(csv.files, csv.importer)
  do.call(rbind, tables)
}

load.DB1.excel <- function(path) { 
  excel.files <- dir(path, pattern = "*.(XL*|xl*)", full.names = TRUE)
  tables <- lapply(excel.files, excel.importer)
  do.call(rbind, tables)
}
```
For example, the `load.DB1.csv` function accepts a path to a folder as the only parameter. The first step creates a list of files that match the pattern. The second step applies a function (the function `csv.importer` defined above) to the list (the `csv.files` list just created). Lastly, this function binds each file together by rows to create a single dataframe of all the data found in the CSV files in a given folder. The `load.DB1.excel` does the same thing for the excel files.

#### The Tidy Dataframe!

After defining the last functions, we can now use these functions to import and tidy all the data from a specified folder.

```{r eval=FALSE, include=FALSE}
DB1.data.csv <- load.DB1.csv(DB1folder)
DB1.data.excel <- load.DB1.excel(DB1folder)

DB1 <- unique(rbind(DB1.data.csv,DB1.data.excel))
```

The final dataframe, called DB1, binds together the CSV and Excel data into a single dataframe. The `unique` operation deletes all duplicate data which might be present in the raw Counter Reports that could be run for arbitary date ranges.

### Transform and Visualize

After getting the usage data and financial information into R and into a tidy data format, my next step was to transform this data. So far, the transformations are limited to filtering, summarizing, and graphing the data. But there are many other transformations that could be considered or added.

Like the other steps, I've adopted a functional approach to handle these transformations. All of the functions described here perform transformations on the same dataframe - the database usage statistics in the tidy format - without ever changing or updating that dataframe.

#### Filtering Usage

Filtering is probably the easiest transformation to understand. Here is the code for a simple function with descriptions.
```{r echo=TRUE}
usage.select.database <- function(DatabaseName,
                                  StartYear,
                                  EndYear,
                                  Action = all.actions){
  DB1 %>%
    filter(Database %in% DatabaseName) %>%
    filter(Date >= StartYear, Date <= EndYear) %>%
    filter(User_Activity %in% Action) %>%
    spread(Date,Usage) %>%
    write_csv(paste(output.folder, "usage.table.1.csv",sep="/")) %>%
    return()
}
```
My function name is r``` usage.select.database ``` - this isn't the best name but I found naming functions to be the most difficult part of the process! This function accepts four parameters and then returns only the relevant usage information.

The parameters:

* ```DatabaseName``` - specifies the database name; right now this must match exactly the name as specified in the Counter Reports.
* ```StartYear``` and ```EndYear``` - takes numerical values that specify the year. This function currently is not sophisticated enough to accept more complicated data parameters (like June 2014 to June 2015) but might be able to in the future.
* ```Action``` - this is an optional parameter that allows the function to filter for a particular User Activity from the Usage Reports. I wanted to make this parameter optional so I supplied the default value of ```all.actions```. This variable was defined earlier - here is the definition ```all.actions <- unique(DB1$User_Activity)``` and while it is not necesary I found it easier to use a varialbe here.

Looking now to the lines in the function, they are very straightforward. The first line filters on the database name, the second line imposes the data limits, and the third line focuses on a particular usage action. Because the original dataframe was tidy, the filter function works wonderfully and the code is very human readable.

However, to make the results more readable to a human, the fourth line 'spreads' the data into a more readable format. The ```spread``` function makes the long (tidy) data into a wider (untidy) format that is somewhat easier to read...or at least more closely approximates the original counter report. After writing this data to a CSV file, this function returns the filtered data in a tabular form.

This function is extremely basic but the general structure of the function as well as the function's parameters form a common structure that makes other functions more readable.

#### Graphing Usage

The next transformation does the same filtering but the output is a simple line graph instead of the raw data. Here is the code for that function:

```{r echo=TRUE}
usage.graph.database <- function(DatabaseName,
                                 StartYear,
                                 EndYear,
                                 Action = all.actions){
  DB1 %>%
    filter(Database %in% DatabaseName) %>%
    filter(Date >= StartYear, Date <= EndYear) %>%
    filter(User_Activity %in% Action) %>%
    ggplot(aes(Date, Usage)) +
    geom_line() +
    geom_smooth(span=0.7) +
    scale_x_yearmon() +
    facet_grid(User_Activity ~ ., scales = "free")
}
```

Again, the four parameters and the first few lines are the same as before. However, instead of returning the data itself, this function uses the ````ggplot``` package to create a graph. The ```ggplot``` package is a very powerful graphics package that can create publication quality graphs. There is a some learning curve to this package and approach, but overall creating a simple line graph like this was relatively straigtforward.

Looking at those lines in detail:

* ```ggplot(aes(Date, Usage))``` - this creates the plot area and specifies the aesthetics that I want to use. Here I'm using Date (on the x axis) and Usage (on the y axis). I'm applying these aesthetics here because I want them to apply for the whole graph.
* ```geom_line()``` - this adds a layer for a line graph to this plot.
* ```geom_smooth(span=0.7) ``` - this adds a layer for a 'smoother'. This helps the graph convey visually the trends that might be otherwise obscured by the great level of variation.
* ```scale_x_yearmon()``` - this specifies that the scale of the x axis is yearmon. This function is from the ```zoo``` package.
* ```facet_grid(User_Activity ~ ., scales = "free")``` - this creates facets which divide the plot into subplots based on the ```User_Activity``` variable. It also specifies that the scales are free because the total usage varies widely based on the different User Actions.

#### Summarizing Usage

The last function I'm going to dissect in this post is awkwardly called ```usage.graph.database.acad.term ```. This function filters usage data based on parameters, mutates the date information to group on academic term, and then returns either the raw data or a graph of the data. Here is the function:

```{r echo=TRUE}
# Graphs database usage based on academic term.
usage.graph.database.acad.term <- function(DatabaseName,StartYear,EndYear,Action = all.actions){
  DB1 %>%
    filter(Database %in% DatabaseName) %>%
    filter(Date >= StartYear, Date <= EndYear) %>%
    filter(User_Activity %in% Action) %>%
    mutate(Year = year(Date), Month=month(Date)) %>%
    mutate(Academic_Term = derivedFactor(
      "Spring" = (Month==1 | Month==2  | Month==3  | Month==4),
      "Summer" = (Month==5 | Month==6  | Month==7  | Month==8),
      "Fall"   = (Month==9 | Month==10 | Month==11 | Month==12)
    )) %>%
    group_by(Database,User_Activity,Academic_Term,Year) %>%
    summarize(Usage=sum(Usage)) %>%
    ggplot(aes(x = Academic_Term, y = Usage)) +
    geom_bar(aes(fill=factor(Year)),
             stat="identity",
             position = position_dodge()) +
    scale_fill_discrete(name="Year") +
    xlab("Academic Term") +
    ylab("Usage")
}
```

Again, the parameters and filtering steps are the same and return only the data we are interested in exploring.

The ```mutate``` lines create new variables from existing data points. ```mutate(Year = year(Date), Month=month(Date)) ``` creates new columns for Year and Month that get the information from the data field. The next chunk adds a new field that derives the academic term from the month. This isn't perfect... but it labels the months of January-April as "Spring", May-August as "Summer", and September-December as "Fall." The function ```derivedFactor``` is from the ```mosaic``` package.

At this point, our data is still tidy but there are additional columns of information containing the Year, Month, and Academic Term. The funciton then groups by Database, User Activity, Academic Term, and Year and then summarizes usage. At this point the data could be spread back to a wider format or graphed in a variety of ways.

This function creates a bar graph of the data with Academic Term on the x axis and Usage on the y axis. The usage is grouped by academic term and color reflects the year. This representation makes it easier to see changes over time by removing some of the variability within the dataset and displaying the information in new ways.

### Communicate
